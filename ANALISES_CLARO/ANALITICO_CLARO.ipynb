{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ANALITICO DE DISCAGENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcoes_claro import *\n",
    "#################################################################################### CARREGANDO TODAS AS DISCAGENS DO MÊS #########################################################\n",
    "caminho = r\"R:\\TI\\TELEFONIA\\BASES CLARO E NET ATIVA\\BASES CLARO\\CHAMADAS_EQUIPE_CLARO\\JULHO_24\"\n",
    "arquivos = glob.glob(caminho + \"/*.csv\")\n",
    "lista = []\n",
    "for arquivo in arquivos:\n",
    "    lista.append(arquivo)\n",
    "    dfList = []\n",
    "    for item in lista:   \n",
    "        chunks = pd.read_csv(item, chunksize=1000000, sep=';', encoding='iso-8859-1', low_memory=False, dtype={'TELEFONE': str})\n",
    "        for df in chunks:\n",
    "            dfList.append(df)\n",
    "# Criando DataFrame com concat e a lista criada a cima\n",
    "discagens_claro_full = pd.concat(dfList, sort=False, ignore_index=False)\n",
    "discagens_claro_full = discagens_claro_full.reset_index(drop=True)\n",
    "\n",
    "### somando os totais diarios para o numero real de discado, atendido, etc\n",
    "discagens_claro_full = discagens_claro_full.groupby(['TELEFONE']).agg({\n",
    "        'DISCADO_SIM':'sum',\n",
    "        'ATENDIDO_SIM': 'sum',\n",
    "        'DTMF_SIM': 'sum',\n",
    "        'ATENDIDO_SIM':'sum'}).reset_index().sort_values(['DISCADO_SIM'])\n",
    "\n",
    "#tratando a coluna TELEFONE\n",
    "discagens_claro_full['TELEFONE'] = discagens_claro_full['TELEFONE'].astype(str).str.strip()\n",
    "\n",
    "### DISCADOS UNICOS E ATENDIDOS UNICOS\n",
    "discagens_claro_full.loc[discagens_claro_full[\"DISCADO_SIM\"] >= 1.0, 'DISCADO_UNICO'] = 1\n",
    "discagens_claro_full.loc[discagens_claro_full[\"ATENDIDO_SIM\"] >= 1.0, 'ATENDIDO_UNICO'] = 1\n",
    "discagens_claro_full['ATENDIDO_UNICO'] = discagens_claro_full['ATENDIDO_UNICO'].fillna(0)\n",
    "discagens_claro_full = discagens_claro_full.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Carregando a base de telefones BLACKLIST\n",
    "blacklist = pd.read_csv(r\"R:\\TI\\TELEFONIA/BLACKLIST_NPTIM.csv\",sep=';',dtype=str)\n",
    "blacklist = blacklist.drop_duplicates(['dddtelefone']).reset_index(drop=True)\n",
    "\n",
    "#tratando a coluna TELEFONE\n",
    "blacklist['dddtelefone'] = blacklist['dddtelefone'].astype(str).str.strip()\n",
    "blacklist = blacklist.rename(columns={'dddtelefone':'TEL_BLACKLIST'})\n",
    "blacklist = blacklist.assign(BLACKLIST = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Carregando a base de telefones validados\n",
    "telefones_validados = pd.read_csv(r\"R:\\TI\\TELEFONIA\\validados_2024/telefones_validados_full.csv\",sep=';',dtype=str)\n",
    "# telefones_validados = telefones_validados.assign(STATUS_TEL = 'Alto_Alô')\n",
    "telefones_validados.rename(columns={'ResultadoClassificacao':'STATUS_TEL'},inplace=True)\n",
    "telefones_validados['dddtelefone'] = telefones_validados['dddtelefone'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### CARREGANDO A BASE PADRONIZADA\n",
    "def carregar_bases_claro_padronizada(caminho, extensao=\"txt\", chunksize=1000000, sep=';', encoding='iso-8859-1'):\n",
    "    try:\n",
    "        # Encontrar todos os arquivos com a extensão especificada no caminho\n",
    "        arquivos = glob.glob(f\"{caminho}/*.{extensao}\")\n",
    "        if not arquivos:\n",
    "            raise FileNotFoundError(f\"Nenhum arquivo '{extensao}' encontrado no caminho especificado: {caminho}\")\n",
    "        \n",
    "        df_list = []\n",
    "\n",
    "        # Iterar sobre cada arquivo encontrado\n",
    "        for arquivo in arquivos:\n",
    "            print(f\"Carregando {arquivo}...\")\n",
    "            # Ler o arquivo em chunks e adicionar à lista de DataFrames\n",
    "            for chunk in pd.read_csv(arquivo, chunksize=chunksize, dtype='str', sep=sep, encoding=encoding):\n",
    "                df_list.append(chunk)\n",
    "\n",
    "        # Concatenar todos os DataFrames em um único DataFrame\n",
    "        bases_concatenadas = pd.concat(df_list, sort=False, ignore_index=True)\n",
    "        print(\"Carregamento e concatenação concluídos com sucesso.\")\n",
    "        return bases_concatenadas\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar arquivos: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "############################################## CARREGANDO AS BASES UNIFICADAS CLARO ##########################################################\n",
    "caminho = r\"R:\\TI\\TELEFONIA\\BASES CLARO E NET ATIVA\\BASES SEMANA PADRAO\"\n",
    "bases_claro_operacional = carregar_bases_claro_padronizada(caminho)\n",
    "\n",
    "bases_claro_operacional['ATRASO'] = bases_claro_operacional['ATRASO'].astype('int16')\n",
    "bases_claro_operacional['SALDO_ABERTO'] = bases_claro_operacional['SALDO_ABERTO'].str.replace('.', '').str.replace(',', '.').astype(float)\n",
    "bases_claro_operacional['BASE'] = bases_claro_operacional['BASE'].apply(lambda x: x.replace('Claro MÃ\\x83Â³vel','Claro Móvel'))\n",
    "\n",
    "\n",
    "#### Filtrando apenas as colunas uteis para uma tabela dimensão\n",
    "bases_claro_operacional = bases_claro_operacional[['CPF','DEVEDOR','TELEFONE','ATRASO','SALDO_ABERTO','BASE','TELEFONE_HIGIENIZADO']]\n",
    "\n",
    "bases_claro_operacional['TELEFONE'] = bases_claro_operacional['TELEFONE'].str.strip()\n",
    "bases_claro_operacional['TELEFONE_HIGIENIZADO'] = bases_claro_operacional['TELEFONE_HIGIENIZADO'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Apartando as bases para termos a base em linhas unicas e nao ter o mesmo telefone na base higienizada e contratante.\n",
    "#### Removendo os duplicados após essa separação.\n",
    "base_contratatante = bases_claro_operacional[['CPF','DEVEDOR','TELEFONE','ATRASO','SALDO_ABERTO','BASE']]\n",
    "base_contratatante = base_contratatante.drop_duplicates(['CPF','DEVEDOR','TELEFONE','ATRASO','BASE'])\n",
    "\n",
    "base_higienizada = bases_claro_operacional[['CPF','DEVEDOR','TELEFONE_HIGIENIZADO','ATRASO','SALDO_ABERTO','BASE']]\n",
    "base_higienizada = base_higienizada.drop_duplicates(['CPF','DEVEDOR','TELEFONE_HIGIENIZADO','ATRASO','BASE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAZENDO OS STATUS DE TELEFONIA, MARCANDO SE É ALTO OU BAIXO ALO\n",
    "base_contratatante = pd.merge(base_contratatante, telefones_validados, right_on=['dddtelefone'], left_on=['TELEFONE'], how=\"left\").loc[:,['CPF','DEVEDOR','TELEFONE','ATRASO','SALDO_ABERTO','BASE','STATUS_TEL']]\n",
    "base_higienizada = pd.merge(base_higienizada, telefones_validados, right_on=['dddtelefone'], left_on=['TELEFONE_HIGIENIZADO'], how=\"left\").loc[:,['CPF','DEVEDOR','TELEFONE_HIGIENIZADO','ATRASO','SALDO_ABERTO','BASE','STATUS_TEL']]\n",
    "\n",
    "base_contratatante['STATUS_TEL'] = base_contratatante['STATUS_TEL'].fillna('nao_validado')\n",
    "base_higienizada['STATUS_TEL'] = base_higienizada['STATUS_TEL'].fillna('nao_validado')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAZENDO AS DISCAGENS PARA A BASE\n",
    "base_contratatante = pd.merge(base_contratatante, discagens_claro_full, right_on=['TELEFONE'], left_on=['TELEFONE'], how=\"left\")\n",
    "\n",
    "### TRAZENDO A BASE QUE ESTÁ EM BLACKLIST\n",
    "base_contratatante = pd.merge(base_contratatante, blacklist, right_on=['TEL_BLACKLIST'], left_on=['TELEFONE'], how=\"left\")\n",
    "\n",
    "base_contratatante = base_contratatante[['CPF', 'DEVEDOR', 'TELEFONE', 'ATRASO', 'SALDO_ABERTO', 'BASE','STATUS_TEL', 'DISCADO_SIM', 'ATENDIDO_SIM', 'DTMF_SIM','DISCADO_UNICO', 'ATENDIDO_UNICO', 'BLACKLIST']]\n",
    "\n",
    "# Contar os caracteres na coluna 'TELEFONE'\n",
    "base_contratatante['NUM_CARACTERES'] = base_contratatante['TELEFONE'].apply(len)\n",
    "\n",
    "# Criar a coluna 'TIPO_TEL' com base no número de caracteres\n",
    "base_contratatante['TIPO_TEL'] = base_contratatante['NUM_CARACTERES'].apply(lambda x: 'CELULAR' if x == 11 else 'FIXO' if x == 10 else 'UNKNOWN')\n",
    "\n",
    "# Remover a coluna auxiliar 'NUM_CARACTERES'\n",
    "base_contratatante = base_contratatante.drop(columns=['NUM_CARACTERES'])\n",
    "\n",
    "base_contratatante[['DISCADO_SIM','ATENDIDO_SIM','DTMF_SIM','DISCADO_UNICO','ATENDIDO_UNICO','BLACKLIST']] = base_contratatante[['DISCADO_SIM','ATENDIDO_SIM','DTMF_SIM','DISCADO_UNICO','ATENDIDO_UNICO','BLACKLIST']].fillna(0)\n",
    "\n",
    "# Criando coluna Faixa de atraso com suas respectivas faixas\n",
    "base_contratatante.loc[base_contratatante.ATRASO.between(0, 90),    'Faixa_de_Atraso'] = \"PDD (0 A 90)\"\n",
    "base_contratatante.loc[base_contratatante.ATRASO.between(91, 360),    'Faixa_de_Atraso'] = \"Atraso (91 a 360)\"\n",
    "base_contratatante.loc[base_contratatante.ATRASO.between(361, 1080),    'Faixa_de_Atraso'] = \"Atraso (361 a 1080)\"\n",
    "base_contratatante.loc[base_contratatante.ATRASO.between(1081, 1800),    'Faixa_de_Atraso'] = \"Atraso (1081 a 1800)\"\n",
    "base_contratatante.loc[base_contratatante.ATRASO.between(1801, 999999),    'Faixa_de_Atraso'] = \"Atraso (Acima 1801)\"\n",
    "\n",
    "### Removendo duplicatas por colunas chaves que nao devem se repetir\n",
    "base_contratatante = base_contratatante.drop_duplicates(['CPF','TELEFONE','BASE','Faixa_de_Atraso']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "############################################## AGRUPANDO O ESTOQUE POR CARTEIRA E POR FAIXA DE ATRASO \n",
    "cpf_unico_por_faixa_de_atraso = base_contratatante[['CPF','BASE',\"Faixa_de_Atraso\"]].drop_duplicates(['CPF','BASE',\"Faixa_de_Atraso\"]).groupby(['BASE','Faixa_de_Atraso']).count().reset_index().rename(columns={'CPF':'CPF_UNICO_POR_FAIXA_ATRASO'})\n",
    "cpf_unico_por_carteira = base_contratatante[['CPF','BASE']].drop_duplicates(['CPF','BASE']).groupby(['BASE']).count().reset_index().rename(columns={'CPF':'CPF_UNICO_POR_CARTEIRA'})\n",
    "\n",
    "total_de_estoque = pd.merge(cpf_unico_por_faixa_de_atraso, cpf_unico_por_carteira, right_on=['BASE'], left_on=['BASE'], how=\"left\")\n",
    "total_de_estoque['CPF_UNICO_POR_CARTEIRA'] = total_de_estoque['CPF_UNICO_POR_CARTEIRA'][total_de_estoque.groupby('BASE')['CPF_UNICO_POR_CARTEIRA'].rank(method='first') == 1]\n",
    "total_de_estoque = total_de_estoque.fillna('')\n",
    "total_de_estoque['CPF_UNICO_POR_CARTEIRA'] = total_de_estoque['CPF_UNICO_POR_CARTEIRA'].astype(str).apply(lambda x: x.replace('.0',''))\n",
    "total_de_estoque = total_de_estoque.rename(columns={'Faixa_de_Atraso':'PERFIL'})\n",
    "\n",
    "\n",
    "#### SEPARANDO OS CPFS QUE FORAM PENETRADOS CONTRATANTES\n",
    "penetracao_cpf_contratante = base_contratatante.query(\"ATENDIDO_SIM >= 1 \")[['BASE','Faixa_de_Atraso','CPF','ATENDIDO_SIM']]\n",
    "penetracao_cpf_contratante = penetracao_cpf_contratante.assign(CPF_PENETRADO = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAZENDO AS DISCAGENS PARA A BASE\n",
    "base_higienizada = pd.merge(base_higienizada, discagens_claro_full, right_on=['TELEFONE'], left_on=['TELEFONE_HIGIENIZADO'], how=\"left\")\n",
    "\n",
    "### TRAZENDO A BASE QUE ESTÁ EM BLACKLIST\n",
    "base_higienizada = pd.merge(base_higienizada, blacklist, right_on=['TEL_BLACKLIST'], left_on=['TELEFONE_HIGIENIZADO'], how=\"left\")\n",
    "\n",
    "base_higienizada = base_higienizada[['CPF', 'DEVEDOR', 'TELEFONE_HIGIENIZADO', 'ATRASO', 'SALDO_ABERTO', 'BASE','STATUS_TEL', 'DISCADO_SIM', 'ATENDIDO_SIM', 'DTMF_SIM','DISCADO_UNICO', 'ATENDIDO_UNICO', 'BLACKLIST']]\n",
    "\n",
    "base_higienizada = base_higienizada.query(\"TELEFONE_HIGIENIZADO.notnull()\")\n",
    "\n",
    "\n",
    "# Contar os caracteres na coluna 'TELEFONE'\n",
    "base_higienizada['NUM_CARACTERES'] = base_higienizada['TELEFONE_HIGIENIZADO'].apply(len)\n",
    "\n",
    "# Criar a coluna 'TIPO_TEL' com base no número de caracteres\n",
    "base_higienizada['TIPO_TEL'] = base_higienizada['NUM_CARACTERES'].apply(lambda x: 'CELULAR' if x == 11 else 'FIXO' if x == 10 else 'UNKNOWN')\n",
    "\n",
    "# Remover a coluna auxiliar 'NUM_CARACTERES'\n",
    "base_higienizada = base_higienizada.drop(columns=['NUM_CARACTERES'])\n",
    "\n",
    "base_higienizada[['DISCADO_SIM','ATENDIDO_SIM','DTMF_SIM','DISCADO_UNICO','ATENDIDO_UNICO','BLACKLIST']] = base_higienizada[['DISCADO_SIM','ATENDIDO_SIM','DTMF_SIM','DISCADO_UNICO','ATENDIDO_UNICO','BLACKLIST']].fillna(0)\n",
    "\n",
    "# Criando coluna Faixa de atraso com suas respectivas faixas\n",
    "base_higienizada.loc[base_higienizada.ATRASO.between(0, 90),    'Faixa_de_Atraso'] = \"PDD (0 A 90)\"\n",
    "base_higienizada.loc[base_higienizada.ATRASO.between(91, 360),    'Faixa_de_Atraso'] = \"Atraso (91 a 360)\"\n",
    "base_higienizada.loc[base_higienizada.ATRASO.between(361, 1080),    'Faixa_de_Atraso'] = \"Atraso (361 a 1080)\"\n",
    "base_higienizada.loc[base_higienizada.ATRASO.between(1081, 1800),    'Faixa_de_Atraso'] = \"Atraso (1081 a 1800)\"\n",
    "base_higienizada.loc[base_higienizada.ATRASO.between(1801, 999999),    'Faixa_de_Atraso'] = \"Atraso (Acima 1801)\"\n",
    "\n",
    "base_higienizada = base_higienizada.drop_duplicates(['CPF','TELEFONE_HIGIENIZADO','BASE','Faixa_de_Atraso']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "#### SEPARANDO OS CPFS QUE FORAM PENETRADOS HIGIENIZADOS\n",
    "penetracao_cpf_higienizada = base_higienizada.query(\"ATENDIDO_SIM >= 1 \")[['BASE','Faixa_de_Atraso','CPF','ATENDIDO_SIM']]\n",
    "penetracao_cpf_higienizada = penetracao_cpf_higienizada.assign(CPF_PENETRADO = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agrupamentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AGRUPAMENTOS\n",
    "\n",
    "### Agrupando discagens \n",
    "group_base_contratatante = base_contratatante.groupby(['BASE','Faixa_de_Atraso']).agg({\n",
    "        'TELEFONE': 'count',\n",
    "        'DISCADO_SIM':'sum',\n",
    "        'DISCADO_UNICO':'sum',\n",
    "        'ATENDIDO_SIM': 'sum',\n",
    "        'ATENDIDO_UNICO':'sum',\n",
    "        'DTMF_SIM':'sum',\n",
    "        'BLACKLIST':'sum'}).reset_index()\n",
    "\n",
    "### criando alguns parametros necessários sobre discagem.\n",
    "group_base_contratatante['%TRAB'] = (group_base_contratatante['DISCADO_UNICO'] / group_base_contratatante['TELEFONE']* 100).round(2)\n",
    "group_base_contratatante['%ATEND_UNICO'] =(group_base_contratatante['ATENDIDO_UNICO'] / group_base_contratatante['DISCADO_UNICO']*100).round(2)\n",
    "group_base_contratatante['SPIN'] = (group_base_contratatante['DISCADO_SIM'] / group_base_contratatante['TELEFONE'].round(2)).round(2)\n",
    "group_base_contratatante['%ALÔ'] = ((group_base_contratatante['ATENDIDO_SIM'] / group_base_contratatante['DISCADO_SIM'])* 100).round(2)\n",
    "group_base_contratatante['%BLACKLIST'] = ((group_base_contratatante['BLACKLIST'] / group_base_contratatante['TELEFONE'])* 100).round(2)\n",
    "\n",
    "\n",
    "group_base_contratatante[[\"DISCADO_SIM\",\"ATENDIDO_SIM\",\"DTMF_SIM\",\"DISCADO_UNICO\",\"ATENDIDO_UNICO\",\"BLACKLIST\"]] = group_base_contratatante[[\"DISCADO_SIM\",\"ATENDIDO_SIM\",\"DTMF_SIM\",\"DISCADO_UNICO\",\"ATENDIDO_UNICO\",\"BLACKLIST\"]].astype('str')\n",
    "\n",
    "group_base_contratatante['DISCADO_SIM'] = group_base_contratatante['DISCADO_SIM'].apply(lambda x: x.replace('.0',''))\n",
    "group_base_contratatante['ATENDIDO_SIM'] = group_base_contratatante['ATENDIDO_SIM'].apply(lambda x: x.replace('.0',''))\n",
    "group_base_contratatante['DTMF_SIM'] = group_base_contratatante['DTMF_SIM'].apply(lambda x: x.replace('.0',''))\n",
    "group_base_contratatante['DISCADO_UNICO'] = group_base_contratatante['DISCADO_UNICO'].apply(lambda x: x.replace('.0',''))\n",
    "group_base_contratatante['ATENDIDO_UNICO'] = group_base_contratatante['ATENDIDO_UNICO'].apply(lambda x: x.replace('.0',''))\n",
    "group_base_contratatante['BLACKLIST'] = group_base_contratatante['BLACKLIST'].apply(lambda x: x.replace('.0',''))\n",
    "\n",
    "group_base_contratatante = group_base_contratatante.rename(columns={'TELEFONE':'TEL EPS','%ALÔ':'%ALÔ EPS' ,'%TRAB':'%TRAB EPS','%BLACKLIST':'%BLACKLIST EPS', 'DISCADO_SIM':'DISCADO EPS', 'DISCADO_UNICO':'DISC UNICO EPS', 'ATENDIDO_SIM':'ATENDIDO EPS','ATENDIDO_UNICO':'ATEND UNICO EPS','DTMF_SIM':'DTMF EPS','Faixa_de_Atraso':'PERFIL','%ATEND_UNICO':'%ATEND UNICO EPS','SPIN':'SPIN EPS'})\n",
    "group_base_contratatante = group_base_contratatante[['BASE','PERFIL','TEL EPS','DISCADO EPS','ATENDIDO EPS','%ALÔ EPS','%ATEND UNICO EPS','%TRAB EPS','%BLACKLIST EPS','DTMF EPS','SPIN EPS']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AGRUPAMENTOS\n",
    "\n",
    "### Agrupando discagens \n",
    "group_base_higienizada = base_higienizada.groupby(['BASE','Faixa_de_Atraso']).agg({\n",
    "        'TELEFONE_HIGIENIZADO': 'count',\n",
    "        'DISCADO_SIM':'sum',\n",
    "        'DISCADO_UNICO':'sum',\n",
    "        'ATENDIDO_SIM': 'sum',\n",
    "        'ATENDIDO_UNICO':'sum',\n",
    "        'DTMF_SIM':'sum',\n",
    "        'BLACKLIST':'sum'}).reset_index()\n",
    "\n",
    "### criando alguns parametros necessários sobre discagem.\n",
    "group_base_higienizada['%TRAB'] = (group_base_higienizada['DISCADO_UNICO'] / group_base_higienizada['TELEFONE_HIGIENIZADO']* 100).round(2)\n",
    "group_base_higienizada['%ATEND_UNICO'] =(group_base_higienizada['ATENDIDO_UNICO'] / group_base_higienizada['DISCADO_UNICO']*100).round(2)\n",
    "group_base_higienizada['SPIN'] = (group_base_higienizada['DISCADO_SIM'] / group_base_higienizada['TELEFONE_HIGIENIZADO'].round(2)).round(2)\n",
    "group_base_higienizada['%ALÔ'] = ((group_base_higienizada['ATENDIDO_SIM'] / group_base_higienizada['DISCADO_SIM'])* 100).round(2)\n",
    "group_base_higienizada['%BLACKLIST'] = ((group_base_higienizada['BLACKLIST'] / group_base_higienizada['TELEFONE_HIGIENIZADO'])* 100).round(2)\n",
    "\n",
    "\n",
    "group_base_higienizada[[\"DISCADO_SIM\",\"ATENDIDO_SIM\",\"DTMF_SIM\",\"DISCADO_UNICO\",\"ATENDIDO_UNICO\",\"BLACKLIST\"]] = group_base_higienizada[[\"DISCADO_SIM\",\"ATENDIDO_SIM\",\"DTMF_SIM\",\"DISCADO_UNICO\",\"ATENDIDO_UNICO\",\"BLACKLIST\"]].astype('str')\n",
    "\n",
    "group_base_higienizada['DISCADO_SIM'] = group_base_higienizada['DISCADO_SIM'].apply(lambda x: x.replace('.0',''))\n",
    "group_base_higienizada['ATENDIDO_SIM'] = group_base_higienizada['ATENDIDO_SIM'].apply(lambda x: x.replace('.0',''))\n",
    "group_base_higienizada['DTMF_SIM'] = group_base_higienizada['DTMF_SIM'].apply(lambda x: x.replace('.0',''))\n",
    "group_base_higienizada['DISCADO_UNICO'] = group_base_higienizada['DISCADO_UNICO'].apply(lambda x: x.replace('.0',''))\n",
    "group_base_higienizada['ATENDIDO_UNICO'] = group_base_higienizada['ATENDIDO_UNICO'].apply(lambda x: x.replace('.0',''))\n",
    "group_base_higienizada['BLACKLIST'] = group_base_higienizada['BLACKLIST'].apply(lambda x: x.replace('.0',''))\n",
    "\n",
    "group_base_higienizada = group_base_higienizada.rename(columns={'TELEFONE_HIGIENIZADO':'TEL HIGI','%ALÔ':'%ALÔ HIGI' ,'%TRAB':'%TRAB HIGI','%BLACKLIST':'%BLACKLIST HIGI', 'DISCADO_SIM':'DISCADO HIGI', 'DISCADO_UNICO':'DISC UNICO HIGI', 'ATENDIDO_SIM':'ATENDIDO HIGI','ATENDIDO_UNICO':'ATEND UNICO HIGI','DTMF_SIM':'DTMF HIGI','Faixa_de_Atraso':'PERFIL','%ATEND_UNICO':'%ATEND UNICO HIGI','SPIN':'SPIN HIGI'})\n",
    "\n",
    "group_base_higienizada = group_base_higienizada[['BASE','PERFIL','TEL HIGI','DISCADO HIGI','ATENDIDO HIGI','%ALÔ HIGI','%ATEND UNICO HIGI','%TRAB HIGI','%BLACKLIST HIGI','DTMF HIGI','SPIN HIGI']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AGRUPANDO AS PENETRAÇÕES POR CPF\n",
    "group_penetração_por_cpf = pd.concat([penetracao_cpf_contratante,penetracao_cpf_higienizada]).drop_duplicates(['Faixa_de_Atraso','CPF']).groupby(['BASE','Faixa_de_Atraso'])['CPF_PENETRADO'].count().reset_index()\n",
    "group_penetração_por_cpf = group_penetração_por_cpf.rename(columns={'Faixa_de_Atraso':'PERFIL'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Analise final\n",
    "group_final = pd.merge(group_base_contratatante, group_base_higienizada, right_on=['BASE','PERFIL'], left_on=['BASE','PERFIL'], how=\"left\")\n",
    "\n",
    "## TRAZENDO O TOTAL DE ESTOQUE e CPF PENETRADO\n",
    "group_final = pd.merge(group_final, total_de_estoque, right_on=['BASE','PERFIL'], left_on=['BASE','PERFIL'], how=\"left\")\n",
    "\n",
    "group_final = pd.merge(group_final, group_penetração_por_cpf, right_on=['BASE','PERFIL'], left_on=['BASE','PERFIL'], how=\"left\")[['BASE','CPF_UNICO_POR_CARTEIRA', 'PERFIL','CPF_UNICO_POR_FAIXA_ATRASO','CPF_PENETRADO', 'TEL EPS', 'DISCADO EPS', 'ATENDIDO EPS', '%ALÔ EPS',\n",
    "       '%ATEND UNICO EPS', '%TRAB EPS', '%BLACKLIST EPS', 'DTMF EPS',\n",
    "       'SPIN EPS', 'TEL HIGI', 'DISCADO HIGI', 'ATENDIDO HIGI', '%ALÔ HIGI',\n",
    "       '%ATEND UNICO HIGI', '%TRAB HIGI', '%BLACKLIST HIGI', 'DTMF HIGI',\n",
    "       'SPIN HIGI']].rename(columns={'CPF_UNICO_POR_CARTEIRA':'CPF_UNICO','CPF_UNICO_POR_FAIXA_ATRASO':'CPF_UNICO_ATR'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Salvando em csv o resultado de discagens\n",
    "group_final.to_csv(r\"C:\\Users\\jorgean.bomfim\\Desktop\\SCRIPTS EM PRODUÇÃO\\CRM CLARO E NET\\ANALISES_CLARO\\DADOS TELES MES VIGENTE\\BASES_E_DISCAGENS_TELES/dados_claro_julho.csv\",sep=';',index=False,encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Certifique-se de que o dataframe 'group_final' esteja carregado no seu ambiente\n",
    "# Por exemplo, se estiver carregando de um arquivo CSV:\n",
    "# group_final = pd.read_csv('caminho/para/seu/arquivo.csv')\n",
    "\n",
    "# Defina as colunas que precisam ser destacadas\n",
    "eps_columns = ['TEL EPS','DISCADO EPS','ATENDIDO EPS','%ALÔ EPS','%ATEND UNICO EPS','%TRAB EPS','%BLACKLIST EPS','DTMF EPS','SPIN EPS']\n",
    "higi_columns = ['TEL HIGI','DISCADO HIGI','ATENDIDO HIGI','%ALÔ HIGI','%ATEND UNICO HIGI','%TRAB HIGI','%BLACKLIST HIGI','DTMF HIGI','SPIN HIGI']\n",
    "\n",
    "\n",
    "# Caminho para salvar o arquivo Excel\n",
    "excel_path = r'C:\\Users\\jorgean.bomfim\\Desktop\\SCRIPTS EM PRODUÇÃO\\CRM CLARO E NET\\ANALISES_CLARO\\DADOS TELES MES VIGENTE\\BASES_E_DISCAGENS_TELES/dados_claro_julho.xlsx'\n",
    "\n",
    "# Salve o dataframe no Excel com formatação\n",
    "with pd.ExcelWriter(excel_path, engine='xlsxwriter') as writer:\n",
    "    group_final.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets['Sheet1']\n",
    "    \n",
    "    # Formatos de cor em hexadecimal (amarelo suave e verde suave) com borda\n",
    "    format_eps = workbook.add_format({'border': 1})  # LemonChiffon\n",
    "    format_higi = workbook.add_format({'border': 1})  # LightGreen\n",
    "    ini_cols = workbook.add_format({'border': 1})\n",
    "    \n",
    "    \n",
    "    # Aplique formatos para colunas específicas e linhas 2 a 13 com borda\n",
    "    for col in eps_columns:\n",
    "        col_idx = group_final.columns.get_loc(col)\n",
    "        worksheet.conditional_format(0, col_idx, 19, col_idx, {'type': 'no_blanks', 'format': format_eps})\n",
    "        \n",
    "    for col in higi_columns:\n",
    "        col_idx = group_final.columns.get_loc(col)\n",
    "        worksheet.conditional_format(0, col_idx, 19, col_idx, {'type': 'no_blanks', 'format': format_higi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acordos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import glob\n",
    "\n",
    "# Configurações e avisos\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Definir colunas necessárias\n",
    "colunas_necessarias = ['Data do Acordo','Negociador','Acordo', 'Carteira', 'Negociador', 'Senha', 'Atraso']\n",
    "\n",
    "# Definir caminho para os arquivos\n",
    "caminho = r\"R:\\TI\\MIS SUP\\ACORDOS_CLARO_NET_GEVENUE\\INTERSIC\"\n",
    "arquivos = glob.glob(caminho + \"/*.txt\")\n",
    "\n",
    "# Inicializar uma lista para armazenar DataFrames\n",
    "dfList = []\n",
    "\n",
    "# Ler e processar cada arquivo\n",
    "for arquivo in arquivos:\n",
    "    # Ler o arquivo inteiro de uma vez\n",
    "    df = pd.read_csv(arquivo, encoding='iso-8859-1', sep=\";\",usecols=colunas_necessarias)\n",
    "    # Adicionar o DataFrame à lista\n",
    "    dfList.append(df)\n",
    "\n",
    "# Concatenar todos os DataFrames em um único DataFrame\n",
    "acordos_intersic = pd.concat(dfList, sort=False, ignore_index=True)\n",
    "\n",
    "# Função para corrigir strings mal codificadas\n",
    "def correct_encoding(text):\n",
    "    if isinstance(text, str):\n",
    "        try:\n",
    "            return text.encode('latin1').decode('utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            return text\n",
    "    return text\n",
    "\n",
    "# Aplicar a função de correção a todas as colunas do DataFrame que são do tipo object (strings)\n",
    "for col in acordos_intersic.select_dtypes(include=[object]).columns:\n",
    "    acordos_intersic['Negociador'] = acordos_intersic['Negociador'].apply(correct_encoding)\n",
    "\n",
    "\n",
    "\n",
    "# acordos_intersic = pd.read_csv(r\"C:\\Users\\jorgean.bomfim\\Desktop\\SCRIPTS EM PRODUÇÃO\\CRM CLARO E NET\\ANALISES_CLARO\\ACORDOS CLARO E NET\\Mis 15_06\\Claro\\Intersic/Acordosporperíodo_19592024115934217263AD9D0F4F.txt\", encoding='iso-8859-1', sep=\";\",usecols=colunas_necessarias)\n",
    "# filtrando oq é atn\n",
    "acordos_intersic = acordos_intersic[acordos_intersic['Negociador'].str.contains('ATN')]\n",
    "\n",
    "# Remover o conteúdo entre parênteses na coluna 'Negociador'\n",
    "acordos_intersic['Negociador'] = acordos_intersic['Negociador'].str.replace(r'\\s*\\([^)]*\\)', '', regex=True)\n",
    "\n",
    "\n",
    "# Definir a função de ajuste\n",
    "def ajustar_carteira(carteira):\n",
    "    if 'TV' in carteira:\n",
    "        return 'Claro TV'\n",
    "    elif 'FIXO' in carteira:\n",
    "        return 'Claro Fixo'\n",
    "    else:\n",
    "        return 'Claro Móvel'\n",
    "\n",
    "# Aplicar a função à coluna 'Carteira'\n",
    "acordos_intersic['Carteira'] = acordos_intersic['Carteira'].apply(ajustar_carteira)\n",
    "acordos_intersic = acordos_intersic.assign(ACORDO = 1)\n",
    "acordos_intersic['Data do Acordo'] = acordos_intersic['Data do Acordo'].str[:10]\n",
    "\n",
    "# Contar a ocorrência de cada contrato\n",
    "contagem_contratos = acordos_intersic['Senha'].value_counts()\n",
    "# Criar a coluna 'ACORDO_UNICO'\n",
    "acordos_intersic['ACORDO_UNICO'] = acordos_intersic['Senha'].apply(lambda x: 1 if contagem_contratos[x] == 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################CARREGANDO OS ACORDOS DA GEVENUE\n",
    "\n",
    "colunas_necessarias_gevenue = ['Data do Acordo ','Negociador','ID Acordo','Produto','Id Gevenue','Aging','Agência']\n",
    "\n",
    "# Definir caminho para os arquivos\n",
    "caminho = r\"R:\\TI\\MIS SUP\\ACORDOS_CLARO_NET_GEVENUE\\GEVENUE\"\n",
    "arquivos = glob.glob(caminho + \"/*.csv\")\n",
    "\n",
    "# Inicializar uma lista para armazenar DataFrames\n",
    "dfList = []\n",
    "\n",
    "# Ler e processar cada arquivo\n",
    "for arquivo in arquivos:\n",
    "    # Ler o arquivo inteiro de uma vez\n",
    "    df = pd.read_csv(arquivo, encoding='utf-8', sep=\"|\",usecols=colunas_necessarias_gevenue)\n",
    "    # Adicionar o DataFrame à lista\n",
    "    dfList.append(df)\n",
    "\n",
    "# Concatenar todos os DataFrames em um único DataFrame\n",
    "acordos_gevenue = pd.concat(dfList, sort=False, ignore_index=True)\n",
    "acordos_gevenue = acordos_gevenue[acordos_gevenue['Agência'].str.contains('ATN')]\n",
    "acordos_gevenue\n",
    "acordos_gevenue.rename(columns={'ID Acordo':'Acordo','Produto':'Carteira','Id Gevenue':'Senha','Aging':'Atraso','Data do Acordo ':'Data do Acordo'},inplace=True)\n",
    "\n",
    "# Definir o mapeamento de valores para substituição\n",
    "replace_dict = {\n",
    "    'ClaroMovel': 'GEVENUE CLARO MOVEL',\n",
    "    'NetTvVirtua': 'GEVENUE NET',\n",
    "    'NetFone': 'GEVENUE NET',\n",
    "    'ClaroFixo': 'GEVENUE CLARO FIXO',\n",
    "}\n",
    "\n",
    "# Aplicar o replace na coluna 'Carteira'\n",
    "acordos_gevenue['Carteira'] = acordos_gevenue['Carteira'].replace(replace_dict)\n",
    "\n",
    "acordos_gevenue = acordos_gevenue.assign(ACORDO = 1)\n",
    "\n",
    "# Contar a ocorrência de cada contrato\n",
    "contagem_contratos = acordos_gevenue['Senha'].value_counts()\n",
    "# Criar a coluna 'ACORDO_UNICO'\n",
    "acordos_gevenue['ACORDO_UNICO'] = acordos_gevenue['Senha'].apply(lambda x: 1 if contagem_contratos[x] == 1 else 0)\n",
    "acordos_gevenue = acordos_gevenue[['Acordo', 'Carteira','Negociador', 'Senha', 'Data do Acordo', 'Atraso', 'ACORDO',  'ACORDO_UNICO']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONCATENANDO OS ACORDOS DOS SISTEMAS INTERSIC E GEVENUE\n",
    "acordos_teles = pd.concat([acordos_intersic,acordos_gevenue])\n",
    "acordos_teles['Atraso'] = acordos_teles['Atraso'].astype('int16')\n",
    "\n",
    "# Criando coluna Faixa de atraso com suas respectivas faixas\n",
    "acordos_teles.loc[acordos_teles.Atraso.between(-9999, -1),    'Faixa_de_Atraso'] = \"Atraso (-1 a -9999)\"\n",
    "acordos_teles.loc[acordos_teles.Atraso.between(0, 90),    'Faixa_de_Atraso'] = \"PDD (0 A 90)\"\n",
    "acordos_teles.loc[acordos_teles.Atraso.between(91, 360),    'Faixa_de_Atraso'] = \"Atraso (91 a 360)\"\n",
    "acordos_teles.loc[acordos_teles.Atraso.between(361, 1080),    'Faixa_de_Atraso'] = \"Atraso (361 a 1080)\"\n",
    "acordos_teles.loc[acordos_teles.Atraso.between(1081, 1800),    'Faixa_de_Atraso'] = \"Atraso (1081 a 1800)\"\n",
    "acordos_teles.loc[acordos_teles.Atraso.between(1801, 999999),    'Faixa_de_Atraso'] = \"Atraso (Acima 1801)\"\n",
    "\n",
    "acordos_teles = acordos_teles.drop(columns=['Senha','Atraso','Acordo']).loc[:,['Data do Acordo','Faixa_de_Atraso','Carteira','Negociador','ACORDO','ACORDO_UNICO']].rename(columns={'Carteira':'BASE'})\n",
    "# Removendo acentos\n",
    "acordos_teles['Negociador'] = acordos_teles['Negociador'].apply(lambda x: unidecode(x))\n",
    "acordos_teles['Negociador'] = acordos_teles['Negociador'].astype(str).str.strip()\n",
    "\n",
    "### BAIXANDO O ANALITICO DOS ACORDOS\n",
    "acordos_teles.to_csv(r\"C:\\Users\\jorgean.bomfim\\Desktop\\SCRIPTS EM PRODUÇÃO\\CRM CLARO E NET\\ANALISES_CLARO\\DADOS TELES MES VIGENTE\\ACORDOS_AGENTES/acordos_teles_agentes_julho.csv\",index=False,sep=';',encoding='iso-8859-1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acionamentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir colunas necessárias\n",
    "colunas_necessarias = ['TIPO_PRODUTO','USU_NOME','DEV_CPF','ACN_DESCRICAO','DEA_DATA','ATRASO']\n",
    "\n",
    "# Definir caminho para os arquivos\n",
    "caminho = r\"R:\\TI\\MIS SUP\\ACIONAMENTOS_CLARO_NET_GEVENUE\\INTERSIC\"\n",
    "arquivos = glob.glob(caminho + \"/*.txt\")\n",
    "\n",
    "# Inicializar uma lista para armazenar DataFrames\n",
    "dfList = []\n",
    "\n",
    "# Ler e processar cada arquivo\n",
    "for arquivo in arquivos:\n",
    "    # Ler o arquivo inteiro de uma vez\n",
    "    df = pd.read_csv(arquivo, encoding='iso-8859-1', sep=\";\",error_bad_lines=False, usecols=colunas_necessarias,dtype={'DEV_CPF':'str'})\n",
    "    # Adicionar o DataFrame à lista\n",
    "    dfList.append(df)\n",
    "\n",
    "# Concatenar todos os DataFrames em um único DataFrame\n",
    "acionamento_intersic = pd.concat(dfList, sort=False, ignore_index=True)\n",
    "\n",
    "# filtrando oq é atn\n",
    "acionamento_intersic = acionamento_intersic[acionamento_intersic['USU_NOME'].str.contains('ATN')]\n",
    "# Remover o conteúdo entre parênteses na coluna 'Negociador'\n",
    "acionamento_intersic['USU_NOME'] = acionamento_intersic['USU_NOME'].str.replace(r'\\s*\\([^)]*\\)', '', regex=True)\n",
    "\n",
    "acionamento_intersic['DEA_DATA'] = acionamento_intersic['DEA_DATA'].str[:10]\n",
    "\n",
    "\n",
    "# Criando coluna Faixa de atraso com suas respectivas faixas\n",
    "acionamento_intersic['ATRASO'] = acionamento_intersic['ATRASO'].astype('int64')\n",
    "acionamento_intersic.loc[acionamento_intersic.ATRASO.between(0, 90),    'Faixa_de_Atraso'] = \"PDD (0 A 90)\"\n",
    "acionamento_intersic.loc[acionamento_intersic.ATRASO.between(91, 360),    'Faixa_de_Atraso'] = \"Atraso (91 a 360)\"\n",
    "acionamento_intersic.loc[acionamento_intersic.ATRASO.between(361, 1080),    'Faixa_de_Atraso'] = \"Atraso (361 a 1080)\"\n",
    "acionamento_intersic.loc[acionamento_intersic.ATRASO.between(1081, 1800),    'Faixa_de_Atraso'] = \"Atraso (1081 a 1800)\"\n",
    "acionamento_intersic.loc[acionamento_intersic.ATRASO.between(1801, 999999),    'Faixa_de_Atraso'] = \"Atraso (Acima 1801)\"\n",
    "\n",
    "## Ajustando layout\n",
    "acionamento_intersic = acionamento_intersic.drop(columns=['ATRASO'])[['DEA_DATA','TIPO_PRODUTO',\t'USU_NOME',\t'DEV_CPF',\t'ACN_DESCRICAO','Faixa_de_Atraso']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir colunas necessárias\n",
    "colunas_necessarias = ['Nome do UsuÃ¡rio','CPF/CNPJ','Aging','Data do Acionamento','Acionamento Feito','Valor Net','Valor Embratel','Valor Movel','Valor Claro TV','Valor Claro Fixo']\n",
    "\n",
    "# Definir caminho para os arquivos\n",
    "caminho = r\"R:\\TI\\MIS SUP\\ACIONAMENTOS_CLARO_NET_GEVENUE\\GEVENUE\"\n",
    "arquivos = glob.glob(caminho + \"/*.csv\")\n",
    "\n",
    "# Inicializar uma lista para armazenar DataFrames\n",
    "dfList = []\n",
    "\n",
    "# Ler e processar cada arquivo\n",
    "for arquivo in arquivos:\n",
    "    # Ler o arquivo inteiro de uma vez\n",
    "    df = pd.read_csv(arquivo, encoding='iso-8859-1', sep=\"|\",error_bad_lines=False, usecols=colunas_necessarias)\n",
    "    # Adicionar o DataFrame à lista\n",
    "    dfList.append(df)\n",
    "\n",
    "# Concatenar todos os DataFrames em um único DataFrame\n",
    "acionamento_gevenue = pd.concat(dfList, sort=False, ignore_index=True)\n",
    "\n",
    "## Remover a aspas do cpf\n",
    "acionamento_gevenue['CPF/CNPJ'] = acionamento_gevenue['CPF/CNPJ'].str.replace(r'\\D', '', regex=True)\n",
    "\n",
    "\n",
    "# Função para determinar os produtos em uma lista\n",
    "def determinar_produto(row):\n",
    "    produtos = []\n",
    "    if pd.notna(row['Valor Net']):\n",
    "        produtos.append('GEVENUE NET')\n",
    "    if pd.notna(row['Valor Embratel']):\n",
    "        produtos.append('GEVENUE NET')\n",
    "    if pd.notna(row['Valor Movel']):\n",
    "        produtos.append('GEVENUE CLARO MOVEL')\n",
    "    if pd.notna(row['Valor Claro TV']):\n",
    "        produtos.append('GEVENUE CLARO TV')\n",
    "    if pd.notna(row['Valor Claro Fixo']):\n",
    "        produtos.append('GEVENUE CLARO FIXO')\n",
    "    return produtos if produtos else None\n",
    "\n",
    "# Aplicar a função ao DataFrame\n",
    "acionamento_gevenue['PRODUTO'] = acionamento_gevenue.apply(determinar_produto, axis=1)\n",
    "\n",
    "# Explodir a coluna PRODUTO para criar linhas separadas\n",
    "acionamento_gevenue = acionamento_gevenue.explode('PRODUTO')\n",
    "\n",
    "## Removendo duplicatas para nao trazer o produto net duas vezes desnecessáriamente\n",
    "acionamento_gevenue = acionamento_gevenue.drop_duplicates(['Data do Acionamento','CPF/CNPJ','PRODUTO']).reset_index(drop=True)\n",
    "\n",
    "# Criando coluna Faixa de atraso com suas respectivas faixas\n",
    "acionamento_gevenue['Aging'] = acionamento_gevenue['Aging'].astype('int64')\n",
    "acionamento_gevenue.loc[acionamento_gevenue.Aging.between(0, 90),    'Faixa_de_Atraso'] = \"PDD (0 A 90)\"\n",
    "acionamento_gevenue.loc[acionamento_gevenue.Aging.between(91, 360),    'Faixa_de_Atraso'] = \"Atraso (91 a 360)\"\n",
    "acionamento_gevenue.loc[acionamento_gevenue.Aging.between(361, 1080),    'Faixa_de_Atraso'] = \"Atraso (361 a 1080)\"\n",
    "acionamento_gevenue.loc[acionamento_gevenue.Aging.between(1081, 1800),    'Faixa_de_Atraso'] = \"Atraso (1081 a 1800)\"\n",
    "acionamento_gevenue.loc[acionamento_gevenue.Aging.between(1801, 999999),    'Faixa_de_Atraso'] = \"Atraso (Acima 1801)\"\n",
    "\n",
    "### ajustando layout\n",
    "acionamento_gevenue = acionamento_gevenue.rename(columns={'Nome do UsuÃ¡rio':'USU_NOME','CPF/CNPJ':'DEV_CPF','Data do Acionamento':'DEA_DATA','Acionamento Feito':'ACN_DESCRICAO','PRODUTO':'TIPO_PRODUTO'})\\\n",
    "[['DEA_DATA','TIPO_PRODUTO',\t'USU_NOME',\t'DEV_CPF',\t'ACN_DESCRICAO','Faixa_de_Atraso']]\n",
    "\n",
    "\n",
    "\n",
    "### Juntando os acionamentos e marcando oq é cpc\n",
    "acionamentos_teles = pd.concat([acionamento_intersic,acionamento_gevenue])\n",
    "acionamentos_teles = acionamentos_teles.assign(ACIONAMENTO = 1)\n",
    "\n",
    "lista_cpc = ['ALEGA PROCESSO JUDICIAL',\n",
    "'ALEGA DESEMPREGO',\n",
    "'ALEGA FRAUDE',\n",
    "'ESTÁ NEGOCIANDO COM O CREDOR',\n",
    "'REENVIO DE BOLETO',\n",
    "'CONTESTAÇÃO',\n",
    "'ALEGA PAGAMENTO',\n",
    "'NÃO RECONHECE A LINHA',\n",
    "'PREVENTIVO REALIZADO',\n",
    "'PROMESSA DE PAGAMENTO',\n",
    "'SEM CONDIÇÕES FINANCEIRAS',\n",
    "'NÃO CONCORDA COM NEGATIVAÇÃO SPC/SERASA',\n",
    "'NÃO CONCORDA VALOR COBRADO',\n",
    "'PROBLEMAS DE SAÚDE',\n",
    "'RECUSA-SE A PAGAR',\n",
    "'Acionamento Ext. - Alega Desemprego',\n",
    "'Acionamento Ext. - Alega Fraude',\n",
    "'Acionamento Ext. - NÃ£o concorda com valor cobrado',\n",
    "'Acionamento Ext. - NÃ£o reconhece a linha',\n",
    "'Acionamento Ext. - Preventivo realizado',\n",
    "'Acionamento Ext. - Problema de saÃºde',\n",
    "'Acionamento Ext. - Promessa de Pagamento',\n",
    "'Acionamento Ext. - Sem CondiÃ§Ãµes Financeiras',\n",
    "'Alega Pagamento',\n",
    "'Alega Processo Judicial',\n",
    "'ContestaÃ§Ã£o',\n",
    "'EstÃ¡ negociando com o Credor',\n",
    "'NÃ£o concorda com NegativaÃ§Ã£o SPC/SERASA',\n",
    "'Recusa-se a pagar']\n",
    "\n",
    "acionamentos_teles['CPC'] = np.where(acionamentos_teles['ACN_DESCRICAO'].isin(lista_cpc), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acionamentos_teles.to_csv(r\"C:\\Users\\jorgean.bomfim\\Desktop\\SCRIPTS EM PRODUÇÃO\\CRM CLARO E NET\\ANALISES_CLARO\\DADOS TELES MES VIGENTE\\ACIONAMENTOS_TELEES/acionamentos_teles_julho.csv\",index=False,sep=';',encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#################################################################################### CARREGANDO TODOS ATENDIMENTOS DO MES (AGENTE) #########################################################\n",
    "caminho = r\"R:\\TI\\TELEFONIA\\BASES CLARO E NET ATIVA\\BASES CLARO\\CHAMADAS_EQUIPE_CLARO\\JULHO_24\\ATENDIDAS AGENTES JULHO\"\n",
    "arquivos = glob.glob(caminho + \"/*.csv\")\n",
    "lista = []\n",
    "for arquivo in arquivos:\n",
    "    lista.append(arquivo)\n",
    "    dfList = []\n",
    "    for item in lista:   \n",
    "        chunks = pd.read_csv(item, chunksize=1000000, sep=';', encoding='iso-8859-1', low_memory=False, dtype= {'CPF_AGENTE':str})\n",
    "        for df in chunks:\n",
    "            dfList.append(df)\n",
    "# Criando DataFrame com concat e a lista criada a cima\n",
    "atendidos_agentes = pd.concat(dfList, sort=False, ignore_index=False)\n",
    "atendidos_agentes = atendidos_agentes.reset_index(drop=True)\n",
    "for col in atendidos_agentes.select_dtypes(include=[object]).columns:\n",
    "    atendidos_agentes['NOME_AGENTE'] = atendidos_agentes['NOME_AGENTE'].apply(correct_encoding)\n",
    "\n",
    "atendidos_agentes = atendidos_agentes[['DATA','CPF_AGENTE','NOME_AGENTE','FILA','ATENDIDAS','PREDITIVO','URA','RECEPTIVO']]\n",
    "\n",
    "\n",
    "# Removendo acentos\n",
    "atendidos_agentes['NOME_AGENTE'] = atendidos_agentes['NOME_AGENTE'].apply(lambda x: unidecode(x))\n",
    "atendidos_agentes['NOME_AGENTE'] = atendidos_agentes['NOME_AGENTE'].astype(str).str.strip()\n",
    "\n",
    "atendidos_agentes.to_csv(r\"C:\\Users\\jorgean.bomfim\\Desktop\\SCRIPTS EM PRODUÇÃO\\CRM CLARO E NET\\ANALISES_CLARO\\DADOS TELES MES VIGENTE\\ATENDIMENTOS_AGENTES/atendidos_teles_agentes_julho.csv\",index=False,sep=';',encoding='iso-8859-1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcoes_claro import *\n",
    "\n",
    "# Carregar o arquivo Excel com a coluna 'CPF_AGENTE' como string\n",
    "info_agentes = pd.read_excel(r'R:\\CARTEIRAS\\COBRANÇA\\CLARO COBRANCA\\MAILINGS/LOGIN X NOME OPERAÇÃO.xlsx',   sheet_name='LOGINS',   dtype={'CPF_AGENTE': str})\n",
    "\n",
    "# Verificar se a coluna 'CPF_AGENTE' está realmente como string\n",
    "info_agentes['CPF AGENTE'] = info_agentes['CPF AGENTE'].astype(str)\n",
    "\n",
    "# Caso haja valores numéricos convertidos, garantir a presença dos zeros à esquerda\n",
    "info_agentes['CPF AGENTE'] = info_agentes['CPF AGENTE'].apply(lambda x: x.zfill(11))\n",
    "\n",
    "\n",
    "###################################################################################\n",
    "### ACORDOS DOS AGENTES MES VIGENTE \n",
    "###################################################################################\n",
    "caminho = r\"C:\\Users\\jorgean.bomfim\\Desktop\\SCRIPTS EM PRODUÇÃO\\CRM CLARO E NET\\ANALISES_CLARO\\DADOS TELES MES VIGENTE\\ACORDOS_AGENTES\"\n",
    "arquivos = glob.glob(caminho + \"/*.csv\")\n",
    "lista = []\n",
    "for arquivo in arquivos:\n",
    "    lista.append(arquivo)\n",
    "    dfList = []\n",
    "    for item in lista:   \n",
    "        chunks = pd.read_csv(item, chunksize=1000000, sep=';', encoding='iso-8859-1', low_memory=False)\n",
    "        for df in chunks:\n",
    "            dfList.append(df)\n",
    "# Criando DataFrame com concat e a lista criada a cima\n",
    "acordos_agentes_vigente = pd.concat(dfList, sort=False, ignore_index=False)\n",
    "\n",
    "# Criando uma coluna chamada FILA, para quando cruzar os acordos dos agentes POR FILA saber onde ele performou melhor, caso seja hibrido.\n",
    "def definir_fila(base):\n",
    "    if pd.notna(base):  # Verifica se a entrada não é NaN\n",
    "        base_lower = base.lower()  # Converte a string para minúsculas\n",
    "        if 'claro' in base_lower:\n",
    "            return 'CLARO'\n",
    "        elif 'net' in base_lower:\n",
    "            return 'NET'\n",
    "    return None\n",
    "\n",
    "# Aplicando a função para criar a coluna FILA\n",
    "acordos_agentes_vigente['FILA'] = acordos_agentes_vigente['BASE'].apply(definir_fila)\n",
    "\n",
    "###################################################################################\n",
    "### ATENDIMENTO DOS AGENTES MES VIGENTE \n",
    "###################################################################################\n",
    "caminho = r\"C:\\Users\\jorgean.bomfim\\Desktop\\SCRIPTS EM PRODUÇÃO\\CRM CLARO E NET\\ANALISES_CLARO\\DADOS TELES MES VIGENTE\\ATENDIMENTOS_AGENTES\"\n",
    "arquivos = glob.glob(caminho + \"/*.csv\")\n",
    "lista = []\n",
    "for arquivo in arquivos:\n",
    "    lista.append(arquivo)\n",
    "    dfList = []\n",
    "    for item in lista:   \n",
    "        chunks = pd.read_csv(item, chunksize=1000000, sep=';', encoding='iso-8859-1', low_memory=False, dtype= {'CPF AGENTE':str})\n",
    "        for df in chunks:\n",
    "            dfList.append(df)\n",
    "# Criando DataFrame com concat e a lista criada a cima\n",
    "atendimentos_agentes_vigente = pd.concat(dfList, sort=False, ignore_index=False)\n",
    "\n",
    "\n",
    "###################################################################################\n",
    "### CARREGANDO ATENDIMENTO DOS AGENTES MES VIGENTE\n",
    "###################################################################################\n",
    "\n",
    "caminho = r\"C:\\Users\\jorgean.bomfim\\Desktop\\SCRIPTS EM PRODUÇÃO\\CRM CLARO E NET\\ANALISES_CLARO\\DADOS TELES MES VIGENTE\\BASES_E_DISCAGENS_TELES\"\n",
    "arquivos = glob.glob(caminho + \"/*.csv\")\n",
    "lista = []\n",
    "for arquivo in arquivos:\n",
    "    lista.append(arquivo)\n",
    "    dfList = []\n",
    "    for item in lista:   \n",
    "        chunks = pd.read_csv(item, chunksize=1000000, sep=';', encoding='iso-8859-1', low_memory=False, dtype= {'CPF_UNICO':str})\n",
    "        for df in chunks:\n",
    "            dfList.append(df)\n",
    "# Criando DataFrame com concat e a lista criada a cima\n",
    "bases_e_discagens_vigente = pd.concat(dfList, sort=False, ignore_index=False)\n",
    "bases_e_discagens_vigente['CPF_UNICO'] = bases_e_discagens_vigente['CPF_UNICO'].fillna('')\n",
    "\n",
    "\n",
    "\n",
    "###################################################################################\n",
    "### CARREGANDO OS ACIONAMENTOS DO MES VIGENTE\n",
    "###################################################################################\n",
    "\n",
    "caminho = r\"C:\\Users\\jorgean.bomfim\\Desktop\\SCRIPTS EM PRODUÇÃO\\CRM CLARO E NET\\ANALISES_CLARO\\DADOS TELES MES VIGENTE\\ACIONAMENTOS_TELEES\"\n",
    "arquivos = glob.glob(caminho + \"/*.csv\")\n",
    "lista = []\n",
    "for arquivo in arquivos:\n",
    "    lista.append(arquivo)\n",
    "    dfList = []\n",
    "    for item in lista:   \n",
    "        chunks = pd.read_csv(item, chunksize=1000000, sep=';', encoding='iso-8859-1', low_memory=False, dtype= {'CPF_UNICO':str})\n",
    "        for df in chunks:\n",
    "            dfList.append(df)\n",
    "# Criando DataFrame com concat e a lista criada a cima\n",
    "acionamentos_vigente = pd.concat(dfList, sort=False, ignore_index=False)\n",
    "acionamentos_vigente = acionamentos_vigente.rename(columns={'TIPO_PRODUTO':'BASE'})\n",
    "# Aplicando a função para criar a coluna FILA\n",
    "acionamentos_vigente['FILA'] = acionamentos_vigente['BASE'].apply(definir_fila)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TRAZENDO A INFORMAÇÃO DE CPF E NOME DO AGENTE\n",
    "acordos_agentes_vigente = pd.merge(acordos_agentes_vigente, info_agentes, right_on=['LOGIN'], left_on=['Negociador'], how=\"left\").drop(columns={'LOGIN'})\n",
    "acordos_agentes_vigente = acordos_agentes_vigente[['Data do Acordo','Faixa_de_Atraso','BASE','FILA','CPF AGENTE','NOME OPERADOR','Negociador','ACORDO','ACORDO_UNICO']]\n",
    "acordos_agentes_vigente = acordos_agentes_vigente.rename(columns={'Negociador':'NOME_SISTEMA'})\n",
    "\n",
    "atendimentos_agentes_vigente = pd.merge(atendimentos_agentes_vigente, info_agentes, right_on=['LOGIN'], left_on=['NOME_AGENTE'], how=\"left\").drop(columns={'LOGIN','CPF_AGENTE'})\n",
    "atendimentos_agentes_vigente = atendimentos_agentes_vigente[['DATA','FILA','CPF AGENTE','NOME OPERADOR','NOME_AGENTE','ATENDIDAS','PREDITIVO','URA','RECEPTIVO']]\n",
    "atendimentos_agentes_vigente = atendimentos_agentes_vigente.rename(columns={'NOME_AGENTE':'NOME_SISTEMA'})\n",
    "\n",
    "acionamentos_vigente = pd.merge(acionamentos_vigente, info_agentes, right_on=['LOGIN'], left_on=['USU_NOME'], how=\"left\").drop(columns={'LOGIN'})\n",
    "acionamentos_vigente = acionamentos_vigente[['DEA_DATA','BASE','Faixa_de_Atraso','FILA','CPF AGENTE','NOME OPERADOR','USU_NOME','DEV_CPF','ACN_DESCRICAO','ACIONAMENTO','CPC']]\n",
    "acionamentos_vigente = acionamentos_vigente.rename(columns={'USU_NOME':'NOME_SISTEMA'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AGRUPANDO OS ACORDOS E DISCAGENS DOS AGENTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_atendimentos_diarios = atendimentos_agentes_vigente.groupby(['DATA','CPF AGENTE','NOME OPERADOR']).agg({\n",
    "        'ATENDIDAS': 'count',\n",
    "        'PREDITIVO':'sum',\n",
    "        'URA':'sum',\n",
    "        'RECEPTIVO': 'sum'}).reset_index()\n",
    "\n",
    "\n",
    "gp_acordos_diarios = acordos_agentes_vigente.groupby(['Data do Acordo','CPF AGENTE','NOME OPERADOR']).agg({\n",
    "        'ACORDO': 'count',\n",
    "        'ACORDO_UNICO':'sum'}).reset_index()\n",
    "\n",
    "########################### AGRUPAMENTO CONSOLIDADO, TODAS AS CHAMADAS E ACORDOS DO AGENTE INDEPENDENTE DA FILA\n",
    "performance_agentes = pd.merge(gp_atendimentos_diarios, gp_acordos_diarios, right_on=['Data do Acordo','CPF AGENTE','NOME OPERADOR'], left_on=['DATA','CPF AGENTE','NOME OPERADOR'], how=\"left\").drop(columns='Data do Acordo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_atendimentos_por_fila = atendimentos_agentes_vigente.groupby(['DATA','CPF AGENTE','NOME OPERADOR','FILA']).agg({\n",
    "        'ATENDIDAS': 'count',\n",
    "        'PREDITIVO':'sum',\n",
    "        'URA':'sum',\n",
    "        'RECEPTIVO': 'sum'}).reset_index()\n",
    "\n",
    "\n",
    "gp_acordos_por_fila = acordos_agentes_vigente.groupby(['Data do Acordo','CPF AGENTE','NOME OPERADOR','FILA']).agg({\n",
    "        'ACORDO': 'count',\n",
    "        'ACORDO_UNICO':'sum'}).reset_index()\n",
    "\n",
    "performance_agentes_por_fila = pd.merge(gp_atendimentos_por_fila, gp_acordos_por_fila, right_on=['Data do Acordo','CPF AGENTE','NOME OPERADOR','FILA'], left_on=['DATA','CPF AGENTE','NOME OPERADOR','FILA'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Acionamento por agente\n",
    "gp_acionamento_agente = acionamentos_vigente.groupby(['DEA_DATA','CPF AGENTE','NOME OPERADOR']).agg({\n",
    "        'ACIONAMENTO': 'sum',\n",
    "        'CPC':'sum'}).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "### Acionamento por agente e fila\n",
    "gp_acionamento_fila = acionamentos_vigente.groupby(['DEA_DATA','CPF AGENTE','NOME OPERADOR','FILA']).agg({\n",
    "        'ACIONAMENTO': 'sum',\n",
    "        'CPC':'sum'}).reset_index()\n",
    "\n",
    "\n",
    "### Acionamento por peril e faixa de atraso\n",
    "gp_acionamento_perfil = acionamentos_vigente.groupby(['BASE','Faixa_de_Atraso']).agg({\n",
    "        'ACIONAMENTO': 'sum',\n",
    "        'CPC':'sum'}).reset_index().rename(columns={'Faixa_de_Atraso':'PERFIL'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### trazendo os acionamentos para a performance do agente\n",
    "performance_agentes = pd.merge(performance_agentes, gp_acionamento_agente, right_on=['DEA_DATA','CPF AGENTE','CPF AGENTE','NOME OPERADOR'], left_on=['DATA','CPF AGENTE','CPF AGENTE','NOME OPERADOR'], how=\"left\").drop(columns='DEA_DATA')\n",
    "\n",
    "### trazendo os acionamentos para a performance do agente por fila\n",
    "performance_agentes_por_fila = performance_agentes_por_fila = pd.merge(performance_agentes_por_fila, gp_acionamento_fila,right_on=['DEA_DATA','CPF AGENTE','CPF AGENTE','NOME OPERADOR','FILA'], left_on=['DATA','CPF AGENTE','CPF AGENTE','NOME OPERADOR','FILA'], how=\"left\").drop(columns=['Data do Acordo','DEA_DATA'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AGRUPANDO OS ACORDOS E DISCAGENS DAS BASES CONSOLIDADAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### AGRUPAMENTO CONSOLIDADO, TODAS AS CHAMADAS E ACORDOS DO AGENTE POR FILA\n",
    "gp_acordos_base_por_faixa_de_atraso = acordos_agentes_vigente.groupby(['BASE','Faixa_de_Atraso']).agg({\n",
    "        'ACORDO': 'count',\n",
    "        'ACORDO_UNICO':'sum'}).reset_index()   \n",
    "\n",
    "bases_e_discagens_vigente = pd.merge(bases_e_discagens_vigente, gp_acordos_base_por_faixa_de_atraso, right_on=['BASE','Faixa_de_Atraso'], left_on=['BASE','PERFIL'], how=\"left\")\n",
    "bases_e_discagens_vigente[[\"ACORDO\",\"ACORDO_UNICO\"]] = bases_e_discagens_vigente[[\"ACORDO\",\"ACORDO_UNICO\"]].astype('str')\n",
    "bases_e_discagens_vigente['ACORDO'] = bases_e_discagens_vigente['ACORDO'].apply(lambda x: x.replace('.0',''))\n",
    "bases_e_discagens_vigente['ACORDO_UNICO'] = bases_e_discagens_vigente['ACORDO_UNICO'].apply(lambda x: x.replace('.0',''))\n",
    "bases_e_discagens_vigente[['ACORDO','ACORDO_UNICO']] = bases_e_discagens_vigente[['ACORDO','ACORDO_UNICO']].apply(lambda x: x.replace('nan',0))\n",
    "bases_e_discagens_vigente = bases_e_discagens_vigente.drop(columns='Faixa_de_Atraso')\n",
    "\n",
    "\n",
    "### trazendo os acionamentos para o relatorio de base e discagens \n",
    "bases_e_discagens_vigente = pd.merge(bases_e_discagens_vigente, gp_acionamento_perfil, right_on=['BASE','PERFIL'], left_on=['BASE','PERFIL'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### AJUSTANDO PRO EXCEL FICAR LIMPO\n",
    "# Função para substituir pontos por vírgulas em colunas numéricas convertidas para strings\n",
    "def replace_decimal_points(df, cols):\n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype(str).str.replace('.', ',')\n",
    "\n",
    "# Aplicar a função no DataFrame\n",
    "bases_e_discagens_vigente_cols_to_replace = ['%ALÔ EPS', '%ATEND UNICO EPS','%TRAB EPS','%BLACKLIST EPS','SPIN EPS','%ALÔ HIGI','%ATEND UNICO HIGI','%TRAB HIGI','%BLACKLIST HIGI','SPIN HIGI']\n",
    "\n",
    "replace_decimal_points(bases_e_discagens_vigente, bases_e_discagens_vigente_cols_to_replace)\n",
    "\n",
    "\n",
    "performance_agentes_por_fila = performance_agentes_por_fila.fillna(0)\n",
    "performance_agentes_por_fila[['ACORDO','ACORDO_UNICO','ACIONAMENTO','CPC']] = performance_agentes_por_fila[['ACORDO','ACORDO_UNICO','ACIONAMENTO','CPC']].astype('int16')\n",
    "\n",
    "performance_agentes = performance_agentes.fillna(0)\n",
    "performance_agentes[['ACORDO','ACORDO_UNICO','ACIONAMENTO','CPC']] = performance_agentes[['ACORDO','ACORDO_UNICO','ACIONAMENTO','CPC']].astype('int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### BAIXANDO TODOS OS AGRUPAMENTOS RESULTADOS FINAIS\n",
    "performance_agentes.to_csv(r\"C:\\Users\\jorgean.bomfim\\Desktop\\SCRIPTS EM PRODUÇÃO\\CRM CLARO E NET\\ANALISES_CLARO\\DADOS TELES MES VIGENTE/performance_agentes_julho.csv\",sep=';',index=False)\n",
    "performance_agentes_por_fila.to_csv(r\"C:\\Users\\jorgean.bomfim\\Desktop\\SCRIPTS EM PRODUÇÃO\\CRM CLARO E NET\\ANALISES_CLARO\\DADOS TELES MES VIGENTE/performance_agentes_por_fila_julho.csv\",sep=';',index=False)\n",
    "bases_e_discagens_vigente.to_csv(r\"C:\\Users\\jorgean.bomfim\\Desktop\\SCRIPTS EM PRODUÇÃO\\CRM CLARO E NET\\ANALISES_CLARO\\DADOS TELES MES VIGENTE/performance_base_julho.csv\",sep=';',index=False, encoding='iso-8859-1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
